\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{authblk}

\title{$\rho$-VAE: \\Autoregressive parametrization of the VAE encoder}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

%\author{  
%  Sohrab Ferdowsi \\
%   \texttt{sohrab.ferdowsi@unige.ch} \\   
% \And 
% Shideh Rezaeifar \\
%  \texttt{shideh.rezaeifar@unige.ch} \\
%   \And
%  Slava Voloshynovkiy\\
%   \texttt{svolos@unige.ch}
%}

\author{\textbf{Sohrab Ferdowsi}}
\author{\textbf{Shideh Rezaeifar}}
\author{\textbf{Maurits Diephuis}}
\author{\textbf{Slava Voloshynovkiy}}

\affil{
Department of Computer Science, University of Geneva, Switzerland \authorcr
  \{\tt sohrab.ferdowsi, shideh.rezaeifar, maurits.diephuis, svolos\}@unige.ch}
  

\begin{document}

\maketitle

\begin{abstract}
We make a simple, but very effective plug-and-play alteration to the standard VAE. This is about parametrization of the approximate posterior of the latent space as a Gaussian distribution whose covariance matrix is constructed as an AR(1) process. We argue that this is a more natural prior for images to consider, as compared to the standard Gaussian distribution with a diagonal covariance matrix. While the standard diagonal parametrization consists of learning the mean vector, as well as the vector of diagonal values for each sample, our parametrization consists of the mean and two scalar values $s$ and $\rho$, where the first one is scaling the distribution and the second one is a measure of correlation among the latent dimensions. Therefore, the performance boost which we show in our experiments to be consistently noticeable cross different setups and variants of VAE models, comes at no cost. We even reduce the number of covariance parameters from $d$, the dimensionality of the latent space to only $2$, yet providing a more flexible approximation of the intractable posterior.

\end{abstract}

\section{Introduction}
Arguably, one of the most successful approaches to representation and generative learning is that of ``Auto-encoding variational Bayes'' \cite{VAE}, a parametrization of the standard variational Bayes in the form of an autoencoder neural network with a recipe for end-to-end learning of its parameters, while providing effective approximation of the intractable posterior. This has then given rise to the very popular Variational AutoEncoder (VAE), a set of models

bla bla

bla bla bla


\section{The VAE models} \label{sec:VAE}
Here we briefly review the standard VAE model, highlighting aspects relevant to our work, as well as some of its further developments.

In a typical probabilistic model where a latent variable $\mathbf{z} \in \Re^d$ is the underlying factor to generate the observable samples $\mathbf{x}$'s $\in \Re^n$, the standard variational Bayes [?] paradigm is concerned with finding an approximation $q(\mathbf{z})$ for the intractable posterior $p(\mathbf{z}|\mathbf{x})$. This is achieved by minimizing the Kullback-Leibler divergence between these two distributions , i.e., $D_{\text{KL}}\Big[ q(z) || p(z|x) \Big]$. Standard treatments of this quantity, along with its non-negativity property will then amount to the following inequality:

\begin{equation}  \label{eq:VB_ELBO}
\log(p(\mathbf{x})) \leqslant \mathbb{E}_{q(\mathbf{z})} \Big[ \log(p(\mathbf{x}|\mathbf{z}))  \Big] - D_{\text{KL}}\Big[ q(\mathbf{z}) || p(\mathbf{z}) \Big].
\end{equation}

\subsection{The standard VAE} \label{subsec:Vanilla_VAE}

Autoencoding variational Bayes \cite{VAE} is then constructing an explicit dependence of the latent variables to the $i^{\text{th}}$ training sample by considering a parametrized distribution $q_{\phi}(\mathbf{z}^{(i)}|\mathbf{x}^{(i)})$ for the approximate posterior, whose construction resembles the encoder part of an autoencoder network with a set of learnable weights $\phi$. Furthermore the training samples can be decoded with $p_{\theta}(\mathbf{x}^{(i)} | \mathbf{z}^{(i)})$, another network with parameters symbolized as $\theta$.

Making this double-sided data dependence more explicit, and by summing over all $N$ training samples results to the following inequality:


\begin{equation} \label{eq:vae_ELBO}
\frac{1}{N}\sum_{i=1}^N \log(p(\mathbf{x}^{(i)})) \leqslant \frac{1}{N} \sum_{i=1}^N \Big[ \log(p_{\theta}(\mathbf{x}^{(i)}|\mathbf{z}^{(i)}))   -  D_{\text{KL}}\big[ q_{\phi}(\mathbf{z}^{(i)}  | \mathbf{x}^{(i)}) || p(\mathbf{z}) \big] \Big].
\end{equation}

This, in fact, is highly relevant for generative modeling as the marginal log-likelihood of the training samples will be upper bounded by two terms, both of which amenable to mini-batch optimization with stochastic gradient descent. 

During optimization, the first term of the LHS can be considered as a data fidelity term, minimized e.g., in the $\ell_2$ sense, since a natural choice for the decoder is $p_{\theta}(\mathbf{x}^{(i)}|\mathbf{z}^{(i)}) = \mathcal{N} \Big( \big( \mathbf{x}^{(i)}|\mathbf{z}^{(i)} \big) , \sigma^2 \mathrm{I}_n \Big)$, where $\mathrm{I}_n$ is the $n$-dimensional unity matrix.

The second term, from the other hand, can be interpreted as a regularization term, pushing the approximate posterior to a prior imposed on the latent space, most conveniently a simple $p(z) = \mathcal{N}(\mathbf{0}, \mathrm{I}_d)$. Provided that the optimization is successful, and the inequality (\ref{eq:ELBO_VAE}) is tight, one can generate random samples from this prior, pass it through the learned decoder and generate samples (non-trivially) similar to the underlying data.   

However, the above scenario comes with a major caution: the fact that sampling $\mathbf{z}$ from $q_{\phi}(\mathbf{z} | \mathbf{x})$ is a non-differentiable operation. The work-around for this issue is the wise ``reparametrization trick'', as proposed in \cite{VAE}. 

The idea is to create the required randomness from a fixed distribution $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathrm{I}_d)$. The samples of the appropriate distribution can then be generated by injecting the learnable moments, e.g., using $\mathbf{z}^{(i)} = \boldsymbol{\mu}^{(i)} + \tilde{\mathrm{C}}^{(i)} \boldsymbol{\epsilon}$, where $\boldsymbol{\mu}^{(i)}$ is the mean vector of the posterior learned for the $i^{\text{th}}$ sample and $\tilde{\mathrm{C}}^{(i)}$ is the Choleskiy decomposition of the corresponding covariance matrix $\mathrm{C}^{(i)}$.

This then limits the practical choices for $\mathrm{C}^{(i)}$ to have analytical Choleskiy decomposition forms, since both $\mathrm{C}^{(i)}$ and $\tilde{\mathrm{C}}^{(i)}$ participate in the optimization simultaneously.

Another issue to address is the calculation of $D_{\text{KL}}\big[ q_{\phi}(\mathbf{z}^{(i)}  | \mathbf{x}^{(i)}) || p(\mathbf{z}) \big]$, which we elaborate slightly more in section \ref{subsec:SOTA}. In order to avoid many practical difficulties, the standard choice is to pick a closed-form expression for it, hence further limiting the choices of $\mathrm{C}^{(i)}$.

While the prior distribution is chosen as $p(z) = \mathcal{N}(\mathbf{0}, \mathrm{I}_d)$, considering the above two constraints, the standard choice widely adopted in many further variants for the sample-wise approximate posterior is to set $\mathrm{C}_{(\mathbf{s})}^{(i)} = \text{diag} \big( \mathbf{s}^{(i)} \big)$. In other words,  $q_{\phi}(\mathbf{z}^{(i)}  | \mathbf{x}^{(i)}) = \mathcal{N} \Big( \boldsymbol{\mu}^{(i)}, \text{diag} \big( \mathbf{s}^{(i)} \big)  \Big)$, a diagonal Gaussian distribution parametrized by the pair$(\boldsymbol{\mu}^{(i)}, \mathbf{s}^{(i)})$.

Note that now, the reparametrization trick can run smoothly, since the Choleskiy decomposition has a closed expression as $\tilde{\mathrm{C}}_{(\mathbf{s})}^{(i)} = \text{diag} \big( \sqrt{\mathbf{s}^{(i)}} \big) $. Furthermore, the regularization term $D_{\text{KL}}\big[ q_{\phi}(\mathbf{z}^{(i)}  | \mathbf{x}^{(i)}) || p(\mathbf{z}) \big] \Big]$ is also calculated analytically as:

\begin{equation}  \label{eq:vae_KLD_loss}
D_{\text{KL}}\Big[ \mathcal{N} \Big( \boldsymbol{\mu}^{(i)}, \text{diag} \big( \mathbf{s}^{(i)} \big) \Big)    \Big|\Big|  \mathcal{N} \big( \mathbf{0}, \mathrm{I}_d \big)  \Big] = \frac{1}{2} \Big[ \mathbf{1}_d^T \mathbf{s}^{(i)} + \big|\big| \boldsymbol{\mu}^{(i)} \big|\big|_2^2 -d -  \mathbf{1}_d^T \log \big({\mathbf{s}^{(i)}} \big) \Big], 
\end{equation}
where $\mathbf{1}_d$ is the unity vector of dimension $d$, $||\cdot||_2^2$ is the squared $\ell_2$-norm, and $\log \big({\mathbf{s}^{(i)}} \big)$ is applied element-wise.

While this is a very practical choice, we argue in section \ref{sec:proposed} that it is too simplistic, as it disregards any correlation within dimensions.


\subsection{Further variations} \label{subsec:SOTA}
The literature around VAE is immense and still very active. Without aiming for any comprehensive literature review, here we still point out several of its variants.

As categorized in \cite{tschannen2018recent}, VAE variants come in 3 categories.
 
\section{The $\rho$-VAE}  \label{sec:proposed}
We saw in section \ref{subsec:Vanilla_VAE} that two considerations limit the choices of approximate posterior: the need for a parametric Choleskiy factorization of its covariance matrix, as well as closed-form expression for the regularization term of (\ref{eq:vae_ELBO}), which basically requires the expression of log-determinant  of the covariance.

In spite of the general consensus to pick $\mathrm{C}_{(\mathbf{s})}^{(i)} = \text{diag}\big( \mathbf{s}^{(i)} \big)$, which does not allow any correlation between the dimensions of the approximate posterior, this work proposes another parametrization that grants such freedom, satisfies the above-mentioned restrictions, and yet has less number of parameters.

In particular, we chose a first-order autoregressive covariance which is characterized by a scaling factor $s$, and another scalar $\rho$ to control the level of correlation, hence the term $\rho$-VAE. This has the form of a simple symmetric Toeplitz matrix as the following:

\begin{equation}  \label{eq:rho_cov}
\mathrm{C}_{(\rho,s)} = s \times  \text{Toeplitz} \Big([1,\rho, \rho^2, \cdots, \rho^{d-1}] \Big)
= s \begin{bmatrix}
    1          & \rho        & \rho^2     & \rho^3       & \cdots   & \rho^{d-1} \\
    \rho       & 1           & \rho       & \rho^2       & \cdots   & \rho^{d-2} \\
    \rho^2     & \rho        & 1          & \rho         & \cdots   & \rho^{d-3} \\
    \rho^3     & \rho^2      & \rho       & 1            & \cdots   & \rho^{d-4} \\
    \vdots     &       &       & \ddots       & \ddots   & \vdots      \\
    \rho^{d-1} & \cdots  & \rho^3 & \rho^2   & \rho   & 1
  \end{bmatrix},
\end{equation}

where $s$ is a positive scalar, and the correlation parameter is bounded as $-1 < \rho  < +1$.

The determinant for this matrix can be calculated as \cite{10.2307/1993228}: 

\begin{equation} \label{eq:rho_det}
\text{det} \big( \mathrm{C}_{(\rho,s)} \big) = s^d (1 - \rho^2)^{d-1},
\end{equation}
based on which we can derive the regularization term of the loss function as:

\begin{equation} \label{eq:rho_KLD_loss}
D_{\text{KL}}\Big[ \mathcal{N} \Big( \boldsymbol{\mu}^{(i)}, \mathrm{C}_{(\rho,s)} \Big)    \Big|\Big|  \mathcal{N} \big( \mathbf{0}, \mathrm{I}_d \big)  \Big] = \frac{1}{2} \Big[
\big|\big| \boldsymbol{\mu}^{(i)} \big|\big|_2^2 + d(s-1-\log{(s)})  - (d-1)\log{(1 - \rho^2)}  
\Big].
\end{equation}


As far as the reparametrization trick is concerned, the Choleskiy decomposition of our choice of covariance matrix has the following lower triangular form:

\begin{equation}    \label{eq:rho_chol}
\tilde{\mathrm{C}}_{(\rho,s)} = \frac{1}{\sqrt{s}} \begin{bmatrix}
    1          & 0        & 0     & 0       & 0   & 0 \\
    \rho       & \sqrt{1-\rho 2}           & 0       & 0       & \cdots   & 0 \\
    \rho^2     & \rho\sqrt{1-\rho 2}        & \sqrt{1-\rho 2}          & 0         & \cdots   & 0 \\
    \rho^3     & \rho^2\sqrt{1-\rho 2}      & \rho\sqrt{1-\rho 2}       & \sqrt{1-\rho 2}           & \cdots   & 0 \\
    \vdots     &       &       & \ddots       & \ddots   & \vdots      \\
    \rho^{d} & \cdots  & \rho^3 \sqrt{1-\rho 2} & \rho^2 \sqrt{1-\rho 2}   & \rho \sqrt{1-\rho 2}   & \sqrt{1-\rho 2}
  \end{bmatrix},
\end{equation}
which can be used to generate the latent codes as $\mathbf{z}^{(i)} = \boldsymbol{\mu}^{(i)} + \tilde{\mathrm{C}}_{(\rho,s)}^{(i)} \boldsymbol{\epsilon}$, which can be constructed also as the element-wise product of $\mathrm{C}_{(\rho,s)}$ with another highly structured matrix.

Otherwise, if depending on the choice of the deep learning framework used, the realization of Toeplitz matrices is not straightforward, one can generate AR(1) samples directly from their definition, i.e., $\mathbf{z}^{(i)}[j] = \boldsymbol{\mu}^{(i)}[j] + \sqrt{s} \boldsymbol{\epsilon}[j] + \rho \mathbf{z}^{(i)}[j-1]$, for $1 < j \leqslant d$.

Although it has less number of parameters than the standard choice and is hence more resilient towards over-fitting, this structure for the approximate posterior is more natural to consider, since correlation will somehow be represented. 

Note that the fact that the prior is chosen as a white Gaussian by design, i.e., $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathrm{I}_d)$, does not obviate the need for the per-sample approximate posterior to account for correlation. In fact, the per-sample posterior can be correlated, yet the aggregation of all samples can be a white Gaussian matching the prior.

Furthermore, the need for correlation does not solely stem from the natural signals like images being correlated. As a matter of fact, another requirement for the success of the VAE-based generative modeling is the tightness of the bound in (\ref{eq:vae_ELBO}), which is controlled by $D_{\text{KL}}\big[ q_{\phi}(\mathbf{z}^{(i)}  | \mathbf{x}^{(i)}) || p_{\theta}(\mathbf{z}^{(i)} | \mathbf{x}^{(i)}) \big]$.

In other words, to guarantee a successful training, the approximate posterior should have enough capacity to match the unknown and intractable posterior. In VAE models, however, it is usually only ``hoped'' that this will be the case. We believe (albeit without providing quantitative evidence), that accounting for correlation may help reduce this gap.

Next we will show the effectiveness of our proposition. We show that the simple alterations to the standard approach, without the need for any sort of hyper-parameter tuning, will noticeably and consistently improve the performance under all variations considered and for all setups. 

\section{Experiments} \label{sec:exp} 



\bibliographystyle{plain}

\bibliography{Bibliography}


\end{document}
